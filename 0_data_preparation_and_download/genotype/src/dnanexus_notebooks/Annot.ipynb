{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","import hail as hl\n","import os\n","import time\n","import dxpy\n","import logging\n","import pandas as pd\n","\n","\n","# Build spark\n","builder = (\n","    SparkSession\n","    .builder\n","    .enableHiveSupport()\n",")\n","spark = builder.getOrCreate()\n","hl.init(sc=spark.sparkContext)\n","\n","\n","\n","vcf_dir = \"/mnt/project/Bulk/Exome sequences/Population level exome OQFE variants, pVCF format - final release/\"\n","chr_num = \"2\"\n","vcf_files = sorted([\"file://\" + os.path.join(vcf_dir, fp) for fp in os.listdir(vcf_dir) if (f\"_c{chr_num}_\" in fp and fp.endswith(\"vcf.gz\"))])\n","\n","\n","\n","def get_rare_variants(mt):\n","    \"\"\"\n","    Returns a matrix table with alt allele frequency < 0.0001\n","    \"\"\"\n","    mt = mt.annotate_rows(gt_stats = hl.agg.call_stats(mt.GT, mt.alleles))\n","    mt = mt.filter_rows((mt.gt_stats.AF[1] < 0.0001) & (mt.gt_stats.AC[1] > 1))\n","    return mt\n","\n","\n","def add_annotations(mt, vep_file=\"file:///mnt/project/exome_annot/annot_run/vep_config_109_v2.json\"):\n","    \"\"\"\n","    Add vep, cadd and dbnsfp annotations\n","    \"\"\"\n","    mt = hl.vep(mt, vep_file) # annot table with vep\n","    db = hl.experimental.DB(region='us', cloud='aws')\n","    mt = db.annotate_rows_db(mt, 'CADD', 'dbNSFP_variants') # add CADD, dbNSFP annotations\n","    return mt\n","\n","\n","def get_protein_coding_variants(mt):\n","    \"\"\"\n","    Search for protein coding transcript consequences.\n","    \"\"\"\n","    mt = mt.filter_rows(hl.any(lambda x: x==\"protein_coding\", mt.vep.transcript_consequences.biotype))\n","    return mt\n","\n","def create_deleteriousness_scores(mt):\n","    metrics = [\"SIFT\", \"LRT\", \"FATHMM\", \"PROVEAN\", \"MetaSVM\", \"MetaLR\", \"PrimateAI\", \"DEOGEN2\"] # \n","    # metrics with D as deleterious and others as tolerant\n","    kwd_dict = {f\"{m}_pred\":hl.if_else(hl.any(lambda x: x.contains(\"D\"), mt.dbNSFP_variants[f\"{m}_pred\"]), 1., 0) for m in metrics}\n","    mt = mt.annotate_rows(**kwd_dict)\n","    # MutationAssessor\n","    mt = mt.annotate_rows(MutationAssessor_pred=hl.if_else(hl.any(lambda x: x.contains(\"H\"), mt.dbNSFP_variants[\"MutationAssessor_pred\"]), 1., 0))\n","    metrics = metrics + [\"MutationAssessor\"]\n","    cols2sum = [f\"{m}_pred\" for m in metrics]\n","    mt = mt.annotate_rows(del_score=hl.sum([mt[col] for col in cols2sum]), cadd=mt.CADD.PHRED_score)\n","    return mt\n","\n","def create_lof_annotation(mt):\n","    lof_mutations = hl.set([\n","        \"transcript_ablation\", \"splice_acceptor_variant\", \"splice_donor_variant\",\n","        \"stop_gained\", \"frameshift_variant\", \"stop_lost\", \"start_lost\"\n","    ])\n","    mt = mt.annotate_rows(\n","        lof = hl.len(lof_mutations.intersection(hl.set(hl.flatten(mt.vep.transcript_consequences.consequence_terms)))) != 0,\n","        loftee_lof = mt.vep.transcript_consequences.lof,\n","        loftee_lof_flag = mt.vep.transcript_consequences.lof_flags,\n","        loftee_lof_filter = mt.vep.transcript_consequences.lof_filter,\n","        loftee_lof_info = mt.vep.transcript_consequences.lof_info\n","    )\n","    return mt\n","\n","def get_deleterious(mt):\n","    # keep all lof (loftee score maybe?) and deleterious missense variants (filter by deleteriousness score, majority vote is 5/9).\n","    # keep cadd greater than ?\n","    mt = mt.filter_rows((mt.lof==True)|(mt.del_score>4))\n","    return mt\n","\n","def add_gene_info(mt):\n","    mt =  mt.annotate_rows(gene=mt.vep.transcript_consequences.gene_symbol)\n","    mt = mt.explode_rows(mt.gene)\n","    return mt\n","\n","def keep_relevant_columns(mt):\n","    mt = mt.select_rows(\n","        mt.del_score, mt.lof, mt.gene, mt.cadd, \n","        mt.loftee_lof, mt.loftee_lof_flag, mt.loftee_lof_filter, mt.loftee_lof_info,\n","    )\n","    return mt\n","\n","def create_burden_matrix(mt):\n","    mt_burden = mt.group_rows_by(mt.gene)\n","    mt_burden = mt_burden.aggregate(n_variants = hl.agg.count_where(mt.GT.n_alt_alleles() > 0))\n","    # filter to genes with at least one rare variant!\n","    mt_burden = mt_burden.filter_rows(hl.agg.sum(mt_burden.n_variants) > 0)\n","    return mt_burden\n","\n","def merge_combine(df1, df2):\n","    df1 = df1.dropna()\n","    df2 = df2.dropna()\n","    df = df1.merge(df2, on=\"gene\", how=\"outer\")\n","    df = df.fillna(\"\")\n","    df[\"samples\"] = df[\"samples_x\"] + \",\" + df[\"samples_y\"]\n","    df = df.drop(columns=[\"samples_x\", \"samples_y\"])\n","    df[\"samples\"] = df.samples.str.strip(\",\")\n","    df[\"samples\"] = df.samples.str.split(\",\").apply(lambda x: \",\".join(set(x)))\n","    return df\n","\n","def get_burden_table(mt):\n","    # split multi-allelic hits to bi-allelic\n","    mt_filtered = hl.split_multi_hts(mt, permit_shuffle=True)\n","    # filter for rare variants only\n","    mt_filtered = get_rare_variants(mt_filtered)\n","    # add vep and dbnsfp annotations\n","    mt_filtered = add_annotations(mt_filtered)\n","    # Only keep protein coding variants\n","    mt_filtered = get_protein_coding_variants(mt_filtered)\n","    # create deleteriousness scores for all variants\n","    mt_filtered = create_deleteriousness_scores(mt_filtered)\n","    # create lof annotations\n","    mt_filtered = create_lof_annotation(mt_filtered)\n","    # filter for lof variants or variants above deletrious score threshold\n","    mt_filtered = get_deleterious(mt_filtered)\n","    # add gene info and explode by gene\n","    mt_filtered = add_gene_info(mt_filtered)\n","    # only keep important columns\n","    mt_filtered = keep_relevant_columns(mt_filtered)\n","    # create burden matrix\n","    burden_mt = create_burden_matrix(mt_filtered)\n","    burden_mt = burden_mt.annotate_rows(samples = hl.bind(lambda x: hl.delimit(x, \",\"), hl.agg.filter(burden_mt.n_variants>0, hl.agg.collect(burden_mt.s))))\n","    # get burden table\n","    burden_table = burden_mt.rows()\n","    burden_df = burden_table.to_pandas()\n","    burden_df = burden_df.dropna()\n","    return burden_df\n","\n","def upload_file_to_project(filename, proj_dir):\n","    dxpy.upload_local_file(filename, folder=proj_dir, parents=True)\n","    print(f\"*********{filename} uploaded!!*********\")\n","    os.remove(filename)\n","    return\n","\n","\n","# Annotation configure\n","logging.basicConfig(filename=f\"chr{chr_num}_annot.log\", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n","\n","burden_df = pd.DataFrame({\"gene\":[], \"samples\":[]})\n","\n","for i in list(range(0, len(vcf_files))):\n","    time_start = time.time()\n","    \n","    # read the matrix table\n","    db_name = f\"exome_chr{chr_num}\"\n","    db_uri = dxpy.find_one_data_object(name=f\"{db_name}\".lower(), classname=\"database\")['id']\n","    mt_name = f\"block_{i}.mt\"\n","    mt_url = f\"dnax://{db_uri}/{mt_name}\"\n","    mt = hl.read_matrix_table(mt_url)\n","    \n","    try:\n","        # create burden table\n","        burden_df = get_burden_table(mt)\n","        # save burden table to local\n","        burden_df_name = f\"block_{i}.tsv\"\n","        burden_df.to_csv(burden_df_name, sep='\\t')\n","        # upload table to project\n","        proj_dir = f\"/exome_annot/annot_run/notebooks/chr{chr_num}/burden_tables_0001/\"\n","        upload_file_to_project(burden_df_name, proj_dir)\n","\n","        time_end = time.time()\n","        time_taken = (time_end - time_start)/60\n","        logging.info(f\"Time to annotate block {i}: {time_taken} mins\\n\")\n","\n","        # remove tmp files created by hail to prevent storage issues \n","        tmp_dir = \"/tmp/\"\n","        for file in os.listdir(tmp_dir):\n","            if file.startswith(\"persist_Table\"):\n","                os.remove(os.path.join(tmp_dir, file))\n","    except Exception as error:\n","        logging.warning(f\"block {i} not annotated due to {error}\\n\")\n","        print(f\"!!!!!!!!block {i} not annotated!!!!!!!!\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
